{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "* We will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
    "* We will do this using an attention model, one of the most sophisticated sequence-to-sequence models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faker is a Python package that generates fake data. Whether we need to bootstrap our database, create good-looking XML documents, fill-in our persistence to stress test it, or anonymize data taken from a production service, Faker is for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Translating human readable dates into machine readable dates\n",
    "\n",
    "* The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. \n",
    "* However, language translation requires massive datasets and usually takes days of training on GPUs. \n",
    "* To give you a place to experiment with these models without using massive datasets, we will perform a simpler \"date translation\" task. \n",
    "* The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) \n",
    "* The network will translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
    "* We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 19067.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wednesday march 20 1996', '1996-03-20'),\n",
       " ('8/15/98', '1998-08-15'),\n",
       " ('03.10.72', '1972-10-03'),\n",
       " ('august 15 1987', '1987-08-15'),\n",
       " ('oct 16 2004', '2004-10-16'),\n",
       " ('25 jan 2003', '2003-01-25'),\n",
       " ('jul 22 1997', '1997-07-22'),\n",
       " ('22.08.90', '1990-08-22'),\n",
       " ('friday june 7 2002', '2002-06-07'),\n",
       " ('saturday may 14 1994', '1994-05-14')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the data and map the raw text data into the index values. \n",
    "- We will set Tx=30 \n",
    "    - We assume Tx is the maximum length of the human readable date.\n",
    "    - If we get a longer input, we would have to truncate it.\n",
    "- We will set Ty=10\n",
    "    - \"YYYY-MM-DD\" is 10 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: wednesday march 20 1996\n",
      "Target date: 1996-03-20\n",
      "\n",
      "Source after preprocessing (indices): [33 17 16 25 17 29 16 13 34  0 24 13 28 15 20  0  5  3  0  4 12 12  9 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  7  0  1  4  0  3  1]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "   \n",
    "Let's implement this neural translator. We will start by implementing two functions: `one_step_attention()` and `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "# This is the post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "    # for the decoder LSTM with shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(units= n_a, return_sequences= True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(inputs= context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(inputs= s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs=[X, s0, c0], outputs= outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimizer \n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer= opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define inputs and outputs, and fit the model\n",
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- We have input X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n",
    "- We need to create `s0` and `c0` to initialize your `post_attention_LSTM_cell` with zeros.\n",
    "- Given the `model()` we coded, we need the \"outputs\" to be a list of 10 elements of shape (m, T_y). \n",
    "    - The list `outputs[i][0], ..., outputs[i][Ty]` represents the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). \n",
    "    - `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2800/10000 [=======>......................] - ETA: 25:14 - loss: 24.0220 - dense_3_loss: 2.3955 - dense_3_acc: 0.0400 - dense_3_acc_1: 0.5200 - dense_3_acc_2: 0.1600 - dense_3_acc_3: 0.0800 - dense_3_acc_4: 0.0000e+00 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0300 - dense_3_acc_7: 0.0000e+00 - dense_3_acc_8: 0.0500 - dense_3_acc_9: 0.11 - ETA: 12:38 - loss: 23.8352 - dense_3_loss: 2.4031 - dense_3_acc: 0.0200 - dense_3_acc_1: 0.2950 - dense_3_acc_2: 0.0850 - dense_3_acc_3: 0.0400 - dense_3_acc_4: 0.4950 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0150 - dense_3_acc_7: 0.4950 - dense_3_acc_8: 0.0250 - dense_3_acc_9: 0.0550       - ETA: 8:26 - loss: 23.6135 - dense_3_loss: 2.4178 - dense_3_acc: 0.0133 - dense_3_acc_1: 0.1967 - dense_3_acc_2: 0.0567 - dense_3_acc_3: 0.0267 - dense_3_acc_4: 0.6633 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0100 - dense_3_acc_7: 0.6633 - dense_3_acc_8: 0.0167 - dense_3_acc_9: 0.0367 - ETA: 6:19 - loss: 23.3680 - dense_3_loss: 2.4489 - dense_3_acc: 0.0100 - dense_3_acc_1: 0.1475 - dense_3_acc_2: 0.0425 - dense_3_acc_3: 0.0200 - dense_3_acc_4: 0.7475 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0075 - dense_3_acc_7: 0.7475 - dense_3_acc_8: 0.0125 - dense_3_acc_9: 0.027 - ETA: 5:03 - loss: 23.1018 - dense_3_loss: 2.5170 - dense_3_acc: 0.0080 - dense_3_acc_1: 0.1180 - dense_3_acc_2: 0.0340 - dense_3_acc_3: 0.0160 - dense_3_acc_4: 0.7980 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0060 - dense_3_acc_7: 0.7980 - dense_3_acc_8: 0.0100 - dense_3_acc_9: 0.022 - ETA: 4:12 - loss: 22.9444 - dense_3_loss: 2.6509 - dense_3_acc: 0.0067 - dense_3_acc_1: 0.0983 - dense_3_acc_2: 0.0283 - dense_3_acc_3: 0.0133 - dense_3_acc_4: 0.8317 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0050 - dense_3_acc_7: 0.8317 - dense_3_acc_8: 0.0083 - dense_3_acc_9: 0.018 - ETA: 3:36 - loss: 22.7670 - dense_3_loss: 2.7259 - dense_3_acc: 0.0057 - dense_3_acc_1: 0.0843 - dense_3_acc_2: 0.0243 - dense_3_acc_3: 0.0114 - dense_3_acc_4: 0.8557 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0043 - dense_3_acc_7: 0.8557 - dense_3_acc_8: 0.0071 - dense_3_acc_9: 0.015 - ETA: 3:09 - loss: 22.6363 - dense_3_loss: 2.7662 - dense_3_acc: 0.0050 - dense_3_acc_1: 0.0737 - dense_3_acc_2: 0.0212 - dense_3_acc_3: 0.0100 - dense_3_acc_4: 0.8738 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0037 - dense_3_acc_7: 0.8738 - dense_3_acc_8: 0.0063 - dense_3_acc_9: 0.013 - ETA: 2:48 - loss: 22.5227 - dense_3_loss: 2.7913 - dense_3_acc: 0.0044 - dense_3_acc_1: 0.1067 - dense_3_acc_2: 0.0189 - dense_3_acc_3: 0.0089 - dense_3_acc_4: 0.8878 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0033 - dense_3_acc_7: 0.8878 - dense_3_acc_8: 0.0056 - dense_3_acc_9: 0.012 - ETA: 2:31 - loss: 22.4058 - dense_3_loss: 2.8003 - dense_3_acc: 0.0040 - dense_3_acc_1: 0.1350 - dense_3_acc_2: 0.0340 - dense_3_acc_3: 0.0080 - dense_3_acc_4: 0.8990 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0030 - dense_3_acc_7: 0.8990 - dense_3_acc_8: 0.0050 - dense_3_acc_9: 0.011 - ETA: 2:17 - loss: 22.3165 - dense_3_loss: 2.8133 - dense_3_acc: 0.0036 - dense_3_acc_1: 0.1609 - dense_3_acc_2: 0.0436 - dense_3_acc_3: 0.0109 - dense_3_acc_4: 0.9082 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0027 - dense_3_acc_7: 0.9082 - dense_3_acc_8: 0.0045 - dense_3_acc_9: 0.010 - ETA: 2:06 - loss: 22.2331 - dense_3_loss: 2.8082 - dense_3_acc: 0.0033 - dense_3_acc_1: 0.1783 - dense_3_acc_2: 0.0517 - dense_3_acc_3: 0.0133 - dense_3_acc_4: 0.9150 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0025 - dense_3_acc_7: 0.9158 - dense_3_acc_8: 0.0042 - dense_3_acc_9: 0.009 - ETA: 1:56 - loss: 22.1678 - dense_3_loss: 2.8015 - dense_3_acc: 0.0031 - dense_3_acc_1: 0.1915 - dense_3_acc_2: 0.0638 - dense_3_acc_3: 0.0200 - dense_3_acc_4: 0.9185 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0023 - dense_3_acc_7: 0.9223 - dense_3_acc_8: 0.0038 - dense_3_acc_9: 0.008 - ETA: 1:47 - loss: 22.0968 - dense_3_loss: 2.7951 - dense_3_acc: 0.0029 - dense_3_acc_1: 0.2057 - dense_3_acc_2: 0.0700 - dense_3_acc_3: 0.0207 - dense_3_acc_4: 0.9243 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0021 - dense_3_acc_7: 0.9279 - dense_3_acc_8: 0.0036 - dense_3_acc_9: 0.007 - ETA: 1:40 - loss: 22.0156 - dense_3_loss: 2.7905 - dense_3_acc: 0.0027 - dense_3_acc_1: 0.2267 - dense_3_acc_2: 0.0813 - dense_3_acc_3: 0.0273 - dense_3_acc_4: 0.9293 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0020 - dense_3_acc_7: 0.9327 - dense_3_acc_8: 0.0033 - dense_3_acc_9: 0.007 - ETA: 1:33 - loss: 21.9374 - dense_3_loss: 2.7844 - dense_3_acc: 0.0025 - dense_3_acc_1: 0.2406 - dense_3_acc_2: 0.0887 - dense_3_acc_3: 0.0331 - dense_3_acc_4: 0.9337 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0019 - dense_3_acc_7: 0.9369 - dense_3_acc_8: 0.0031 - dense_3_acc_9: 0.006 - ETA: 1:28 - loss: 21.8700 - dense_3_loss: 2.7835 - dense_3_acc: 0.0024 - dense_3_acc_1: 0.2429 - dense_3_acc_2: 0.0924 - dense_3_acc_3: 0.0359 - dense_3_acc_4: 0.9376 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0018 - dense_3_acc_7: 0.9406 - dense_3_acc_8: 0.0029 - dense_3_acc_9: 0.006 - ETA: 1:22 - loss: 21.7992 - dense_3_loss: 2.7836 - dense_3_acc: 0.0022 - dense_3_acc_1: 0.2478 - dense_3_acc_2: 0.0961 - dense_3_acc_3: 0.0417 - dense_3_acc_4: 0.9411 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0017 - dense_3_acc_7: 0.9439 - dense_3_acc_8: 0.0028 - dense_3_acc_9: 0.006 - ETA: 1:18 - loss: 21.7280 - dense_3_loss: 2.7889 - dense_3_acc: 0.0021 - dense_3_acc_1: 0.2568 - dense_3_acc_2: 0.1021 - dense_3_acc_3: 0.0453 - dense_3_acc_4: 0.9442 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0016 - dense_3_acc_7: 0.9468 - dense_3_acc_8: 0.0026 - dense_3_acc_9: 0.005 - ETA: 1:14 - loss: 21.6685 - dense_3_loss: 2.7975 - dense_3_acc: 0.0020 - dense_3_acc_1: 0.2745 - dense_3_acc_2: 0.1055 - dense_3_acc_3: 0.0470 - dense_3_acc_4: 0.9470 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0015 - dense_3_acc_7: 0.9495 - dense_3_acc_8: 0.0025 - dense_3_acc_9: 0.005 - ETA: 1:10 - loss: 21.6111 - dense_3_loss: 2.8072 - dense_3_acc: 0.0019 - dense_3_acc_1: 0.2924 - dense_3_acc_2: 0.1062 - dense_3_acc_3: 0.0500 - dense_3_acc_4: 0.9495 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0014 - dense_3_acc_7: 0.9519 - dense_3_acc_8: 0.0024 - dense_3_acc_9: 0.005 - ETA: 1:07 - loss: 21.5369 - dense_3_loss: 2.8023 - dense_3_acc: 0.0018 - dense_3_acc_1: 0.3100 - dense_3_acc_2: 0.1114 - dense_3_acc_3: 0.0527 - dense_3_acc_4: 0.9518 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0014 - dense_3_acc_7: 0.9541 - dense_3_acc_8: 0.0023 - dense_3_acc_9: 0.005 - ETA: 1:03 - loss: 21.4678 - dense_3_loss: 2.8017 - dense_3_acc: 0.0017 - dense_3_acc_1: 0.3265 - dense_3_acc_2: 0.1183 - dense_3_acc_3: 0.0548 - dense_3_acc_4: 0.9539 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0013 - dense_3_acc_7: 0.9561 - dense_3_acc_8: 0.0022 - dense_3_acc_9: 0.004 - ETA: 1:00 - loss: 21.4023 - dense_3_loss: 2.7974 - dense_3_acc: 0.0017 - dense_3_acc_1: 0.3396 - dense_3_acc_2: 0.1225 - dense_3_acc_3: 0.0537 - dense_3_acc_4: 0.9558 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0012 - dense_3_acc_7: 0.9579 - dense_3_acc_8: 0.0158 - dense_3_acc_9: 0.009 - ETA: 58s - loss: 21.3392 - dense_3_loss: 2.7927 - dense_3_acc: 0.0016 - dense_3_acc_1: 0.3488 - dense_3_acc_2: 0.1240 - dense_3_acc_3: 0.0524 - dense_3_acc_4: 0.9576 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0012 - dense_3_acc_7: 0.9596 - dense_3_acc_8: 0.0152 - dense_3_acc_9: 0.015 - ETA: 55s - loss: 21.2746 - dense_3_loss: 2.7898 - dense_3_acc: 0.0015 - dense_3_acc_1: 0.3604 - dense_3_acc_2: 0.1304 - dense_3_acc_3: 0.0504 - dense_3_acc_4: 0.9592 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0012 - dense_3_acc_7: 0.9612 - dense_3_acc_8: 0.0146 - dense_3_acc_9: 0.01 - ETA: 53s - loss: 21.2103 - dense_3_loss: 2.7875 - dense_3_acc: 0.0015 - dense_3_acc_1: 0.3696 - dense_3_acc_2: 0.1326 - dense_3_acc_3: 0.0485 - dense_3_acc_4: 0.9607 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0011 - dense_3_acc_7: 0.9626 - dense_3_acc_8: 0.0141 - dense_3_acc_9: 0.01 - ETA: 51s - loss: 21.1552 - dense_3_loss: 2.7867 - dense_3_acc: 0.0014 - dense_3_acc_1: 0.3779 - dense_3_acc_2: 0.1336 - dense_3_acc_3: 0.0468 - dense_3_acc_4: 0.9621 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0011 - dense_3_acc_7: 0.9586 - dense_3_acc_8: 0.0275 - dense_3_acc_9: 0.0182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5600/10000 [===============>..............] - ETA: 49s - loss: 21.0959 - dense_3_loss: 2.7892 - dense_3_acc: 0.0014 - dense_3_acc_1: 0.3866 - dense_3_acc_2: 0.1366 - dense_3_acc_3: 0.0452 - dense_3_acc_4: 0.9634 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0010 - dense_3_acc_7: 0.9576 - dense_3_acc_8: 0.0386 - dense_3_acc_9: 0.02 - ETA: 47s - loss: 21.0381 - dense_3_loss: 2.7877 - dense_3_acc: 0.0013 - dense_3_acc_1: 0.3923 - dense_3_acc_2: 0.1383 - dense_3_acc_3: 0.0437 - dense_3_acc_4: 0.9647 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 1.0000e-03 - dense_3_acc_7: 0.9590 - dense_3_acc_8: 0.0373 - dense_3_acc_9: 0.02 - ETA: 45s - loss: 20.9802 - dense_3_loss: 2.7884 - dense_3_acc: 0.0148 - dense_3_acc_1: 0.3997 - dense_3_acc_2: 0.1416 - dense_3_acc_3: 0.0468 - dense_3_acc_4: 0.9658 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 9.6774e-04 - dense_3_acc_7: 0.9603 - dense_3_acc_8: 0.0361 - dense_3_acc_9: 0.02 - ETA: 43s - loss: 20.9217 - dense_3_loss: 2.7873 - dense_3_acc: 0.0337 - dense_3_acc_1: 0.4066 - dense_3_acc_2: 0.1453 - dense_3_acc_3: 0.0469 - dense_3_acc_4: 0.9669 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 9.3750e-04 - dense_3_acc_7: 0.9616 - dense_3_acc_8: 0.0409 - dense_3_acc_9: 0.02 - ETA: 42s - loss: 20.8630 - dense_3_loss: 2.7844 - dense_3_acc: 0.0518 - dense_3_acc_1: 0.4133 - dense_3_acc_2: 0.1464 - dense_3_acc_3: 0.0455 - dense_3_acc_4: 0.9679 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 9.0909e-04 - dense_3_acc_7: 0.9627 - dense_3_acc_8: 0.0485 - dense_3_acc_9: 0.03 - ETA: 40s - loss: 20.8107 - dense_3_loss: 2.7843 - dense_3_acc: 0.0671 - dense_3_acc_1: 0.4179 - dense_3_acc_2: 0.1476 - dense_3_acc_3: 0.0441 - dense_3_acc_4: 0.9688 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 8.8235e-04 - dense_3_acc_7: 0.9638 - dense_3_acc_8: 0.0526 - dense_3_acc_9: 0.03 - ETA: 39s - loss: 20.7562 - dense_3_loss: 2.7839 - dense_3_acc: 0.0803 - dense_3_acc_1: 0.4211 - dense_3_acc_2: 0.1497 - dense_3_acc_3: 0.0460 - dense_3_acc_4: 0.9697 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 8.5714e-04 - dense_3_acc_7: 0.9649 - dense_3_acc_8: 0.0511 - dense_3_acc_9: 0.03 - ETA: 37s - loss: 20.7008 - dense_3_loss: 2.7859 - dense_3_acc: 0.0944 - dense_3_acc_1: 0.4258 - dense_3_acc_2: 0.1514 - dense_3_acc_3: 0.0500 - dense_3_acc_4: 0.9706 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 8.3333e-04 - dense_3_acc_7: 0.9658 - dense_3_acc_8: 0.0497 - dense_3_acc_9: 0.03 - ETA: 36s - loss: 20.6479 - dense_3_loss: 2.7833 - dense_3_acc: 0.1078 - dense_3_acc_1: 0.4303 - dense_3_acc_2: 0.1527 - dense_3_acc_3: 0.0508 - dense_3_acc_4: 0.9714 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 8.1081e-04 - dense_3_acc_7: 0.9668 - dense_3_acc_8: 0.0541 - dense_3_acc_9: 0.04 - ETA: 35s - loss: 20.5955 - dense_3_loss: 2.7857 - dense_3_acc: 0.1197 - dense_3_acc_1: 0.4337 - dense_3_acc_2: 0.1539 - dense_3_acc_3: 0.0539 - dense_3_acc_4: 0.9721 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 7.8947e-04 - dense_3_acc_7: 0.9676 - dense_3_acc_8: 0.0613 - dense_3_acc_9: 0.04 - ETA: 33s - loss: 20.5401 - dense_3_loss: 2.7858 - dense_3_acc: 0.1315 - dense_3_acc_1: 0.4374 - dense_3_acc_2: 0.1579 - dense_3_acc_3: 0.0533 - dense_3_acc_4: 0.9728 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 7.6923e-04 - dense_3_acc_7: 0.9685 - dense_3_acc_8: 0.0695 - dense_3_acc_9: 0.04 - ETA: 32s - loss: 20.4924 - dense_3_loss: 2.7845 - dense_3_acc: 0.1425 - dense_3_acc_1: 0.4407 - dense_3_acc_2: 0.1607 - dense_3_acc_3: 0.0527 - dense_3_acc_4: 0.9735 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 7.5000e-04 - dense_3_acc_7: 0.9693 - dense_3_acc_8: 0.0680 - dense_3_acc_9: 0.05 - ETA: 31s - loss: 20.4405 - dense_3_loss: 2.7858 - dense_3_acc: 0.1554 - dense_3_acc_1: 0.4463 - dense_3_acc_2: 0.1627 - dense_3_acc_3: 0.0534 - dense_3_acc_4: 0.9741 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 7.3171e-04 - dense_3_acc_7: 0.9700 - dense_3_acc_8: 0.0663 - dense_3_acc_9: 0.05 - ETA: 30s - loss: 20.3903 - dense_3_loss: 2.7857 - dense_3_acc: 0.1643 - dense_3_acc_1: 0.4483 - dense_3_acc_2: 0.1636 - dense_3_acc_3: 0.0524 - dense_3_acc_4: 0.9748 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 7.1429e-04 - dense_3_acc_7: 0.9707 - dense_3_acc_8: 0.0648 - dense_3_acc_9: 0.05 - ETA: 29s - loss: 20.3356 - dense_3_loss: 2.7837 - dense_3_acc: 0.1751 - dense_3_acc_1: 0.4526 - dense_3_acc_2: 0.1681 - dense_3_acc_3: 0.0519 - dense_3_acc_4: 0.9753 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.9767e-04 - dense_3_acc_7: 0.9714 - dense_3_acc_8: 0.0702 - dense_3_acc_9: 0.05 - ETA: 28s - loss: 20.2865 - dense_3_loss: 2.7807 - dense_3_acc: 0.1848 - dense_3_acc_1: 0.4559 - dense_3_acc_2: 0.1700 - dense_3_acc_3: 0.0511 - dense_3_acc_4: 0.9759 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.8182e-04 - dense_3_acc_7: 0.9718 - dense_3_acc_8: 0.0770 - dense_3_acc_9: 0.05 - ETA: 27s - loss: 20.2347 - dense_3_loss: 2.7791 - dense_3_acc: 0.1949 - dense_3_acc_1: 0.4600 - dense_3_acc_2: 0.1727 - dense_3_acc_3: 0.0509 - dense_3_acc_4: 0.9764 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.6667e-04 - dense_3_acc_7: 0.9724 - dense_3_acc_8: 0.0829 - dense_3_acc_9: 0.05 - ETA: 26s - loss: 20.1872 - dense_3_loss: 2.7787 - dense_3_acc: 0.2033 - dense_3_acc_1: 0.4626 - dense_3_acc_2: 0.1748 - dense_3_acc_3: 0.0502 - dense_3_acc_4: 0.9770 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.5217e-04 - dense_3_acc_7: 0.9730 - dense_3_acc_8: 0.0820 - dense_3_acc_9: 0.05 - ETA: 26s - loss: 20.1366 - dense_3_loss: 2.7766 - dense_3_acc: 0.2115 - dense_3_acc_1: 0.4653 - dense_3_acc_2: 0.1779 - dense_3_acc_3: 0.0496 - dense_3_acc_4: 0.9774 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.3830e-04 - dense_3_acc_7: 0.9736 - dense_3_acc_8: 0.0826 - dense_3_acc_9: 0.06 - ETA: 25s - loss: 20.0906 - dense_3_loss: 2.7780 - dense_3_acc: 0.2204 - dense_3_acc_1: 0.4690 - dense_3_acc_2: 0.1802 - dense_3_acc_3: 0.0494 - dense_3_acc_4: 0.9779 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.2500e-04 - dense_3_acc_7: 0.9742 - dense_3_acc_8: 0.0871 - dense_3_acc_9: 0.06 - ETA: 24s - loss: 20.0403 - dense_3_loss: 2.7774 - dense_3_acc: 0.2288 - dense_3_acc_1: 0.4722 - dense_3_acc_2: 0.1837 - dense_3_acc_3: 0.0488 - dense_3_acc_4: 0.9784 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.1224e-04 - dense_3_acc_7: 0.9737 - dense_3_acc_8: 0.0939 - dense_3_acc_9: 0.06 - ETA: 23s - loss: 19.9952 - dense_3_loss: 2.7773 - dense_3_acc: 0.2352 - dense_3_acc_1: 0.4738 - dense_3_acc_2: 0.1858 - dense_3_acc_3: 0.0478 - dense_3_acc_4: 0.9788 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 6.0000e-04 - dense_3_acc_7: 0.9742 - dense_3_acc_8: 0.0956 - dense_3_acc_9: 0.06 - ETA: 22s - loss: 19.9464 - dense_3_loss: 2.7750 - dense_3_acc: 0.2410 - dense_3_acc_1: 0.4749 - dense_3_acc_2: 0.1876 - dense_3_acc_3: 0.0475 - dense_3_acc_4: 0.9792 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.8824e-04 - dense_3_acc_7: 0.9747 - dense_3_acc_8: 0.0945 - dense_3_acc_9: 0.06 - ETA: 22s - loss: 19.8983 - dense_3_loss: 2.7741 - dense_3_acc: 0.2483 - dense_3_acc_1: 0.4777 - dense_3_acc_2: 0.1894 - dense_3_acc_3: 0.0471 - dense_3_acc_4: 0.9796 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.7692e-04 - dense_3_acc_7: 0.9752 - dense_3_acc_8: 0.0935 - dense_3_acc_9: 0.06 - ETA: 21s - loss: 19.8537 - dense_3_loss: 2.7711 - dense_3_acc: 0.2549 - dense_3_acc_1: 0.4800 - dense_3_acc_2: 0.1911 - dense_3_acc_3: 0.0483 - dense_3_acc_4: 0.9800 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.6604e-04 - dense_3_acc_7: 0.9757 - dense_3_acc_8: 0.0957 - dense_3_acc_9: 0.06 - ETA: 20s - loss: 19.8001 - dense_3_loss: 2.7678 - dense_3_acc: 0.2620 - dense_3_acc_1: 0.4830 - dense_3_acc_2: 0.1939 - dense_3_acc_3: 0.0489 - dense_3_acc_4: 0.9804 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.5556e-04 - dense_3_acc_7: 0.9754 - dense_3_acc_8: 0.1009 - dense_3_acc_9: 0.06 - ETA: 20s - loss: 19.7596 - dense_3_loss: 2.7688 - dense_3_acc: 0.2682 - dense_3_acc_1: 0.4851 - dense_3_acc_2: 0.1945 - dense_3_acc_3: 0.0480 - dense_3_acc_4: 0.9807 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.4545e-04 - dense_3_acc_7: 0.9758 - dense_3_acc_8: 0.1062 - dense_3_acc_9: 0.06 - ETA: 19s - loss: 19.7180 - dense_3_loss: 2.7676 - dense_3_acc: 0.2725 - dense_3_acc_1: 0.4855 - dense_3_acc_2: 0.1968 - dense_3_acc_3: 0.0471 - dense_3_acc_4: 0.9811 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.3571e-04 - dense_3_acc_7: 0.9763 - dense_3_acc_8: 0.1086 - dense_3_acc_9: 0.0711"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8500/10000 [========================>.....] - ETA: 18s - loss: 19.6694 - dense_3_loss: 2.7659 - dense_3_acc: 0.2789 - dense_3_acc_1: 0.4882 - dense_3_acc_2: 0.1995 - dense_3_acc_3: 0.0467 - dense_3_acc_4: 0.9814 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.2632e-04 - dense_3_acc_7: 0.9767 - dense_3_acc_8: 0.1067 - dense_3_acc_9: 0.07 - ETA: 18s - loss: 19.6235 - dense_3_loss: 2.7628 - dense_3_acc: 0.2843 - dense_3_acc_1: 0.4900 - dense_3_acc_2: 0.2012 - dense_3_acc_3: 0.0466 - dense_3_acc_4: 0.9817 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 5.1724e-04 - dense_3_acc_7: 0.9771 - dense_3_acc_8: 0.1100 - dense_3_acc_9: 0.07 - ETA: 17s - loss: 19.5748 - dense_3_loss: 2.7598 - dense_3_acc: 0.2900 - dense_3_acc_1: 0.4927 - dense_3_acc_2: 0.2037 - dense_3_acc_3: 0.0463 - dense_3_acc_4: 0.9820 - dense_3_acc_5: 0.0025 - dense_3_acc_6: 5.0847e-04 - dense_3_acc_7: 0.9710 - dense_3_acc_8: 0.1151 - dense_3_acc_9: 0.0744   - ETA: 17s - loss: 19.5256 - dense_3_loss: 2.7588 - dense_3_acc: 0.2965 - dense_3_acc_1: 0.4962 - dense_3_acc_2: 0.2043 - dense_3_acc_3: 0.0458 - dense_3_acc_4: 0.9823 - dense_3_acc_5: 0.0052 - dense_3_acc_6: 6.6667e-04 - dense_3_acc_7: 0.9712 - dense_3_acc_8: 0.1195 - dense_3_acc_9: 0.07 - ETA: 16s - loss: 19.4727 - dense_3_loss: 2.7584 - dense_3_acc: 0.3025 - dense_3_acc_1: 0.4998 - dense_3_acc_2: 0.2084 - dense_3_acc_3: 0.0454 - dense_3_acc_4: 0.9826 - dense_3_acc_5: 0.0077 - dense_3_acc_6: 6.5574e-04 - dense_3_acc_7: 0.9716 - dense_3_acc_8: 0.1200 - dense_3_acc_9: 0.07 - ETA: 15s - loss: 19.4231 - dense_3_loss: 2.7558 - dense_3_acc: 0.3065 - dense_3_acc_1: 0.5008 - dense_3_acc_2: 0.2116 - dense_3_acc_3: 0.0452 - dense_3_acc_4: 0.9829 - dense_3_acc_5: 0.0098 - dense_3_acc_6: 6.4516e-04 - dense_3_acc_7: 0.9721 - dense_3_acc_8: 0.1182 - dense_3_acc_9: 0.07 - ETA: 15s - loss: 19.3690 - dense_3_loss: 2.7536 - dense_3_acc: 0.3105 - dense_3_acc_1: 0.5025 - dense_3_acc_2: 0.2129 - dense_3_acc_3: 0.0444 - dense_3_acc_4: 0.9832 - dense_3_acc_5: 0.0138 - dense_3_acc_6: 6.3492e-04 - dense_3_acc_7: 0.9725 - dense_3_acc_8: 0.1189 - dense_3_acc_9: 0.07 - ETA: 14s - loss: 19.3164 - dense_3_loss: 2.7517 - dense_3_acc: 0.3147 - dense_3_acc_1: 0.5047 - dense_3_acc_2: 0.2150 - dense_3_acc_3: 0.0441 - dense_3_acc_4: 0.9834 - dense_3_acc_5: 0.0223 - dense_3_acc_6: 0.0017 - dense_3_acc_7: 0.9730 - dense_3_acc_8: 0.1252 - dense_3_acc_9: 0.0806   - ETA: 14s - loss: 19.2581 - dense_3_loss: 2.7463 - dense_3_acc: 0.3202 - dense_3_acc_1: 0.5091 - dense_3_acc_2: 0.2175 - dense_3_acc_3: 0.0448 - dense_3_acc_4: 0.9837 - dense_3_acc_5: 0.0326 - dense_3_acc_6: 0.0032 - dense_3_acc_7: 0.9734 - dense_3_acc_8: 0.1294 - dense_3_acc_9: 0.08 - ETA: 13s - loss: 19.2018 - dense_3_loss: 2.7470 - dense_3_acc: 0.3247 - dense_3_acc_1: 0.5121 - dense_3_acc_2: 0.2198 - dense_3_acc_3: 0.0453 - dense_3_acc_4: 0.9839 - dense_3_acc_5: 0.0420 - dense_3_acc_6: 0.0047 - dense_3_acc_7: 0.9738 - dense_3_acc_8: 0.1352 - dense_3_acc_9: 0.08 - ETA: 13s - loss: 19.1436 - dense_3_loss: 2.7454 - dense_3_acc: 0.3293 - dense_3_acc_1: 0.5163 - dense_3_acc_2: 0.2225 - dense_3_acc_3: 0.0446 - dense_3_acc_4: 0.9842 - dense_3_acc_5: 0.0500 - dense_3_acc_6: 0.0049 - dense_3_acc_7: 0.9742 - dense_3_acc_8: 0.1379 - dense_3_acc_9: 0.08 - ETA: 12s - loss: 19.0872 - dense_3_loss: 2.7449 - dense_3_acc: 0.3331 - dense_3_acc_1: 0.5203 - dense_3_acc_2: 0.2250 - dense_3_acc_3: 0.0441 - dense_3_acc_4: 0.9844 - dense_3_acc_5: 0.0594 - dense_3_acc_6: 0.0059 - dense_3_acc_7: 0.9746 - dense_3_acc_8: 0.1409 - dense_3_acc_9: 0.08 - ETA: 12s - loss: 19.0208 - dense_3_loss: 2.7429 - dense_3_acc: 0.3374 - dense_3_acc_1: 0.5233 - dense_3_acc_2: 0.2262 - dense_3_acc_3: 0.0443 - dense_3_acc_4: 0.9846 - dense_3_acc_5: 0.0668 - dense_3_acc_6: 0.0077 - dense_3_acc_7: 0.9749 - dense_3_acc_8: 0.1441 - dense_3_acc_9: 0.08 - ETA: 11s - loss: 18.9528 - dense_3_loss: 2.7386 - dense_3_acc: 0.3413 - dense_3_acc_1: 0.5263 - dense_3_acc_2: 0.2281 - dense_3_acc_3: 0.0439 - dense_3_acc_4: 0.9849 - dense_3_acc_5: 0.0731 - dense_3_acc_6: 0.0091 - dense_3_acc_7: 0.9753 - dense_3_acc_8: 0.1477 - dense_3_acc_9: 0.08 - ETA: 11s - loss: 18.8862 - dense_3_loss: 2.7360 - dense_3_acc: 0.3451 - dense_3_acc_1: 0.5308 - dense_3_acc_2: 0.2301 - dense_3_acc_3: 0.0439 - dense_3_acc_4: 0.9851 - dense_3_acc_5: 0.0790 - dense_3_acc_6: 0.0104 - dense_3_acc_7: 0.9756 - dense_3_acc_8: 0.1503 - dense_3_acc_9: 0.08 - ETA: 10s - loss: 18.8112 - dense_3_loss: 2.7315 - dense_3_acc: 0.3492 - dense_3_acc_1: 0.5350 - dense_3_acc_2: 0.2315 - dense_3_acc_3: 0.0440 - dense_3_acc_4: 0.9853 - dense_3_acc_5: 0.0862 - dense_3_acc_6: 0.0124 - dense_3_acc_7: 0.9760 - dense_3_acc_8: 0.1540 - dense_3_acc_9: 0.08 - ETA: 10s - loss: 18.7409 - dense_3_loss: 2.7270 - dense_3_acc: 0.3515 - dense_3_acc_1: 0.5370 - dense_3_acc_2: 0.2333 - dense_3_acc_3: 0.0453 - dense_3_acc_4: 0.9855 - dense_3_acc_5: 0.0945 - dense_3_acc_6: 0.0127 - dense_3_acc_7: 0.9763 - dense_3_acc_8: 0.1573 - dense_3_acc_9: 0.08 - ETA: 9s - loss: 18.6678 - dense_3_loss: 2.7244 - dense_3_acc: 0.3547 - dense_3_acc_1: 0.5411 - dense_3_acc_2: 0.2354 - dense_3_acc_3: 0.0469 - dense_3_acc_4: 0.9857 - dense_3_acc_5: 0.1041 - dense_3_acc_6: 0.0136 - dense_3_acc_7: 0.9766 - dense_3_acc_8: 0.1592 - dense_3_acc_9: 0.0896 - ETA: 9s - loss: 18.5926 - dense_3_loss: 2.7205 - dense_3_acc: 0.3585 - dense_3_acc_1: 0.5455 - dense_3_acc_2: 0.2380 - dense_3_acc_3: 0.0475 - dense_3_acc_4: 0.9859 - dense_3_acc_5: 0.1128 - dense_3_acc_6: 0.0141 - dense_3_acc_7: 0.9769 - dense_3_acc_8: 0.1616 - dense_3_acc_9: 0.090 - ETA: 8s - loss: 18.5146 - dense_3_loss: 2.7165 - dense_3_acc: 0.3612 - dense_3_acc_1: 0.5497 - dense_3_acc_2: 0.2395 - dense_3_acc_3: 0.0480 - dense_3_acc_4: 0.9861 - dense_3_acc_5: 0.1217 - dense_3_acc_6: 0.0147 - dense_3_acc_7: 0.9772 - dense_3_acc_8: 0.1651 - dense_3_acc_9: 0.090 - ETA: 8s - loss: 18.4395 - dense_3_loss: 2.7123 - dense_3_acc: 0.3635 - dense_3_acc_1: 0.5527 - dense_3_acc_2: 0.2401 - dense_3_acc_3: 0.0487 - dense_3_acc_4: 0.9862 - dense_3_acc_5: 0.1305 - dense_3_acc_6: 0.0152 - dense_3_acc_7: 0.9775 - dense_3_acc_8: 0.1679 - dense_3_acc_9: 0.091 - ETA: 8s - loss: 18.3629 - dense_3_loss: 2.7073 - dense_3_acc: 0.3665 - dense_3_acc_1: 0.5568 - dense_3_acc_2: 0.2423 - dense_3_acc_3: 0.0496 - dense_3_acc_4: 0.9864 - dense_3_acc_5: 0.1385 - dense_3_acc_6: 0.0164 - dense_3_acc_7: 0.9778 - dense_3_acc_8: 0.1712 - dense_3_acc_9: 0.091 - ETA: 7s - loss: 18.2850 - dense_3_loss: 2.7028 - dense_3_acc: 0.3701 - dense_3_acc_1: 0.5606 - dense_3_acc_2: 0.2442 - dense_3_acc_3: 0.0506 - dense_3_acc_4: 0.9866 - dense_3_acc_5: 0.1453 - dense_3_acc_6: 0.0186 - dense_3_acc_7: 0.9781 - dense_3_acc_8: 0.1739 - dense_3_acc_9: 0.092 - ETA: 7s - loss: 18.2096 - dense_3_loss: 2.6981 - dense_3_acc: 0.3736 - dense_3_acc_1: 0.5641 - dense_3_acc_2: 0.2465 - dense_3_acc_3: 0.0516 - dense_3_acc_4: 0.9867 - dense_3_acc_5: 0.1532 - dense_3_acc_6: 0.0200 - dense_3_acc_7: 0.9784 - dense_3_acc_8: 0.1770 - dense_3_acc_9: 0.092 - ETA: 6s - loss: 18.1330 - dense_3_loss: 2.6929 - dense_3_acc: 0.3765 - dense_3_acc_1: 0.5679 - dense_3_acc_2: 0.2477 - dense_3_acc_3: 0.0526 - dense_3_acc_4: 0.9869 - dense_3_acc_5: 0.1606 - dense_3_acc_6: 0.0216 - dense_3_acc_7: 0.9786 - dense_3_acc_8: 0.1795 - dense_3_acc_9: 0.093 - ETA: 6s - loss: 18.0569 - dense_3_loss: 2.6891 - dense_3_acc: 0.3796 - dense_3_acc_1: 0.5717 - dense_3_acc_2: 0.2487 - dense_3_acc_3: 0.0538 - dense_3_acc_4: 0.9871 - dense_3_acc_5: 0.1684 - dense_3_acc_6: 0.0222 - dense_3_acc_7: 0.9789 - dense_3_acc_8: 0.1834 - dense_3_acc_9: 0.093 - ETA: 5s - loss: 17.9859 - dense_3_loss: 2.6858 - dense_3_acc: 0.3823 - dense_3_acc_1: 0.5754 - dense_3_acc_2: 0.2505 - dense_3_acc_3: 0.0549 - dense_3_acc_4: 0.9872 - dense_3_acc_5: 0.1745 - dense_3_acc_6: 0.0237 - dense_3_acc_7: 0.9792 - dense_3_acc_8: 0.1866 - dense_3_acc_9: 0.093 - ETA: 5s - loss: 17.9148 - dense_3_loss: 2.6810 - dense_3_acc: 0.3854 - dense_3_acc_1: 0.5788 - dense_3_acc_2: 0.2530 - dense_3_acc_3: 0.0563 - dense_3_acc_4: 0.9874 - dense_3_acc_5: 0.1819 - dense_3_acc_6: 0.0243 - dense_3_acc_7: 0.9794 - dense_3_acc_8: 0.1885 - dense_3_acc_9: 0.094 - ETA: 5s - loss: 17.8411 - dense_3_loss: 2.6767 - dense_3_acc: 0.3899 - dense_3_acc_1: 0.5822 - dense_3_acc_2: 0.2547 - dense_3_acc_3: 0.0567 - dense_3_acc_4: 0.9875 - dense_3_acc_5: 0.1891 - dense_3_acc_6: 0.0254 - dense_3_acc_7: 0.9796 - dense_3_acc_8: 0.1928 - dense_3_acc_9: 0.0942"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 4s - loss: 17.7719 - dense_3_loss: 2.6729 - dense_3_acc: 0.3930 - dense_3_acc_1: 0.5857 - dense_3_acc_2: 0.2558 - dense_3_acc_3: 0.0573 - dense_3_acc_4: 0.9877 - dense_3_acc_5: 0.1949 - dense_3_acc_6: 0.0267 - dense_3_acc_7: 0.9799 - dense_3_acc_8: 0.1958 - dense_3_acc_9: 0.094 - ETA: 4s - loss: 17.7022 - dense_3_loss: 2.6690 - dense_3_acc: 0.3977 - dense_3_acc_1: 0.5893 - dense_3_acc_2: 0.2567 - dense_3_acc_3: 0.0583 - dense_3_acc_4: 0.9878 - dense_3_acc_5: 0.2020 - dense_3_acc_6: 0.0282 - dense_3_acc_7: 0.9801 - dense_3_acc_8: 0.1985 - dense_3_acc_9: 0.094 - ETA: 4s - loss: 17.6331 - dense_3_loss: 2.6650 - dense_3_acc: 0.4012 - dense_3_acc_1: 0.5931 - dense_3_acc_2: 0.2584 - dense_3_acc_3: 0.0591 - dense_3_acc_4: 0.9880 - dense_3_acc_5: 0.2081 - dense_3_acc_6: 0.0299 - dense_3_acc_7: 0.9803 - dense_3_acc_8: 0.1999 - dense_3_acc_9: 0.094 - ETA: 3s - loss: 17.5642 - dense_3_loss: 2.6609 - dense_3_acc: 0.4062 - dense_3_acc_1: 0.5960 - dense_3_acc_2: 0.2593 - dense_3_acc_3: 0.0599 - dense_3_acc_4: 0.9881 - dense_3_acc_5: 0.2145 - dense_3_acc_6: 0.0319 - dense_3_acc_7: 0.9806 - dense_3_acc_8: 0.2024 - dense_3_acc_9: 0.095 - ETA: 3s - loss: 17.4955 - dense_3_loss: 2.6574 - dense_3_acc: 0.4109 - dense_3_acc_1: 0.5996 - dense_3_acc_2: 0.2610 - dense_3_acc_3: 0.0609 - dense_3_acc_4: 0.9882 - dense_3_acc_5: 0.2201 - dense_3_acc_6: 0.0336 - dense_3_acc_7: 0.9808 - dense_3_acc_8: 0.2046 - dense_3_acc_9: 0.095 - ETA: 3s - loss: 17.4301 - dense_3_loss: 2.6535 - dense_3_acc: 0.4147 - dense_3_acc_1: 0.6023 - dense_3_acc_2: 0.2621 - dense_3_acc_3: 0.0611 - dense_3_acc_4: 0.9884 - dense_3_acc_5: 0.2259 - dense_3_acc_6: 0.0346 - dense_3_acc_7: 0.9810 - dense_3_acc_8: 0.2064 - dense_3_acc_9: 0.096 - ETA: 2s - loss: 17.3647 - dense_3_loss: 2.6501 - dense_3_acc: 0.4195 - dense_3_acc_1: 0.6058 - dense_3_acc_2: 0.2630 - dense_3_acc_3: 0.0622 - dense_3_acc_4: 0.9885 - dense_3_acc_5: 0.2317 - dense_3_acc_6: 0.0363 - dense_3_acc_7: 0.9812 - dense_3_acc_8: 0.2088 - dense_3_acc_9: 0.096 - ETA: 2s - loss: 17.3015 - dense_3_loss: 2.6466 - dense_3_acc: 0.4242 - dense_3_acc_1: 0.6083 - dense_3_acc_2: 0.2637 - dense_3_acc_3: 0.0632 - dense_3_acc_4: 0.9886 - dense_3_acc_5: 0.2371 - dense_3_acc_6: 0.0381 - dense_3_acc_7: 0.9814 - dense_3_acc_8: 0.2103 - dense_3_acc_9: 0.096 - ETA: 1s - loss: 17.2353 - dense_3_loss: 2.6427 - dense_3_acc: 0.4284 - dense_3_acc_1: 0.6118 - dense_3_acc_2: 0.2655 - dense_3_acc_3: 0.0643 - dense_3_acc_4: 0.9887 - dense_3_acc_5: 0.2427 - dense_3_acc_6: 0.0394 - dense_3_acc_7: 0.9816 - dense_3_acc_8: 0.2116 - dense_3_acc_9: 0.098 - ETA: 1s - loss: 17.1708 - dense_3_loss: 2.6388 - dense_3_acc: 0.4327 - dense_3_acc_1: 0.6149 - dense_3_acc_2: 0.2679 - dense_3_acc_3: 0.0647 - dense_3_acc_4: 0.9888 - dense_3_acc_5: 0.2480 - dense_3_acc_6: 0.0409 - dense_3_acc_7: 0.9818 - dense_3_acc_8: 0.2132 - dense_3_acc_9: 0.098 - ETA: 1s - loss: 17.1071 - dense_3_loss: 2.6356 - dense_3_acc: 0.4375 - dense_3_acc_1: 0.6180 - dense_3_acc_2: 0.2703 - dense_3_acc_3: 0.0656 - dense_3_acc_4: 0.9890 - dense_3_acc_5: 0.2532 - dense_3_acc_6: 0.0422 - dense_3_acc_7: 0.9820 - dense_3_acc_8: 0.2155 - dense_3_acc_9: 0.098 - ETA: 0s - loss: 17.0393 - dense_3_loss: 2.6316 - dense_3_acc: 0.4428 - dense_3_acc_1: 0.6215 - dense_3_acc_2: 0.2721 - dense_3_acc_3: 0.0671 - dense_3_acc_4: 0.9891 - dense_3_acc_5: 0.2590 - dense_3_acc_6: 0.0437 - dense_3_acc_7: 0.9822 - dense_3_acc_8: 0.2180 - dense_3_acc_9: 0.099 - ETA: 0s - loss: 16.9790 - dense_3_loss: 2.6283 - dense_3_acc: 0.4478 - dense_3_acc_1: 0.6248 - dense_3_acc_2: 0.2734 - dense_3_acc_3: 0.0681 - dense_3_acc_4: 0.9892 - dense_3_acc_5: 0.2643 - dense_3_acc_6: 0.0451 - dense_3_acc_7: 0.9823 - dense_3_acc_8: 0.2195 - dense_3_acc_9: 0.099 - ETA: 0s - loss: 16.9176 - dense_3_loss: 2.6250 - dense_3_acc: 0.4529 - dense_3_acc_1: 0.6282 - dense_3_acc_2: 0.2752 - dense_3_acc_3: 0.0688 - dense_3_acc_4: 0.9893 - dense_3_acc_5: 0.2696 - dense_3_acc_6: 0.0474 - dense_3_acc_7: 0.9825 - dense_3_acc_8: 0.2209 - dense_3_acc_9: 0.099 - 32s 3ms/step - loss: 16.8577 - dense_3_loss: 2.6219 - dense_3_acc: 0.4577 - dense_3_acc_1: 0.6313 - dense_3_acc_2: 0.2762 - dense_3_acc_3: 0.0696 - dense_3_acc_4: 0.9894 - dense_3_acc_5: 0.2752 - dense_3_acc_6: 0.0494 - dense_3_acc_7: 0.9827 - dense_3_acc_8: 0.2231 - dense_3_acc_9: 0.1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207fa337978>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run this model for longer, and saved the weights. Load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "\n",
    "# EXAMPLES = ['3 July 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "# for example in EXAMPLES:\n",
    "    \n",
    "#     source = string_to_int(example, Tx, human_vocab)\n",
    "#     source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "#     prediction = model.predict([source, s0, c0])\n",
    "#     prediction = np.argmax(prediction, axis = -1)\n",
    "#     output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "#     print(\"source:\", example)\n",
    "#     print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20791c5d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGsCAYAAAD9ro91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xm8HXV5+PHPcxMgC4vssklQWQQqgYRFcV+poqJiFbeqqK3br2r1Vy2ttYvWra210ipUS93ArVbl51JEBRMJmyYQEAUFFKQqyGJCFpL7/P6YueTk5sycc5dz7ze5n/frdZJz5jvfmefMzLnPmTkz80RmIkmSyjU03QFIkqR2JmtJkgpnspYkqXAma0mSCmeyliSpcCZrSZIKZ7KWJKlwJmtJkgpnspYkqXCzpzuATnvssUceeOCCrm2rV69m/vz545ruTOq7tcVr37Ln2U/f9Rub74K4fs1qtp/b3Pean97W2PbAXXfgf+9c19i+8JB9G9vuXb2KefN3bGyPxhZpat18803cfvvtPTfJopL1gQcuYOmlV3Rtu2TJd3nEox43runOpL5bW7z2LXue/fS97a61jW3XL7+Egxc+orH98Oe+t7HtLS88hDM+85PG9ou++ZeNbZdfcjHHPuIxje3bzfagospw4vGL+xrPLVaSpMKZrCVJKtzAknVEfDwifh0RKwc1D0mSZoJB7lmfA5w0wOlLkjQjDCxZZ+bFwG8HNX1JkmaKyGy+7GLCE49YAJyfmUe2jPNq4NUAe++996Jzzzuv63irVq1ixx2bL8VoM5P6bm3x2rfsefbT974NzX9D1q5ZxZy5zX1X/vR/G9v2230Hbr2j7dKtfRrbVq9axfyWmMNrt1SIt/zpW7jyyivKv3QrM88CzgJYtGhxNl0iUuplK6X13dritW/Z8+yn70Qu3XrW3zZfuvWuHpdu/fqbz29s89ItbWvcYiVJKpzJWpKkwg3y0q1zgUuAQyPilog4fVDzkiRpWzaw36wz87RBTVuSpJnEw+CSJBXOZC1JUuGm/dItSVu3Xedv19g2a1a0tjOn5drvoaHW9lvvXNPYtn7DcGv7gj3HVy5Umi7uWUuSVDiTtSRJhTNZS5JUuIEm64j4k4hYGRHXRMQbBzkvSZK2VYO8KcqRwKuA44CjgJMj4uBBzU+SpG3VIPesHwYsy8x7M3MDcBHw7AHOT5KkbdLASmRGxMOALwOPANYAFwJXZOYbRo1nicxJ7Lu1xWvfsufZT9/hlj8h965axbyWvitu+FVj2367bs+td65vbD98wZ6NbevXrmb7Oc2XZ+2wnafrqAzTXiIzM38UEe8FLgBWASuADV3Gs0TmJPbd2uK1b9nz7Kfv2vs2NrZduex7LDrh0Y3tJ7/ng41t7zp1f874wi2N7T885+TGtptWXsqCI49vbPc6a21tBvr1MjM/lpnHZOZjgN8C1w9yfpIkbYsGegeziNgrM38dEQ8CnkN1SFySJI3BoG83+sWI2B24D3hdZt454PlJkrTNGWiyzszmH6skSVJfPCVSkqTCmawlSSqcJTIlTch9G4Yb2zKztZ27/re5bePere277bh9Y9sts6K1XdrauGctSVLhTNaSJBWur2QdEQdGxJPq53MjYqfBhiVJkkb0TNYR8SrgC8BH60H7A//dz8QtkSlJ0sT1s2f9OuBE4B6AzLwe2KtXJ0tkSpI0OfpJ1usy8/7SNxExG+inVJclMiVJmgQ9S2RGxPuAu4CXAm8AXgtcm5ln9OhniUzLKNp3EvuWGu/GlhqZa1avYu785r5X/eTWxrb9dp/HrXfc29j+8EP2G/d8Zw31rEgoTYl+S2T2k6yHgNOBpwABfBP49+yjEHZEnE51GH0VcC2wJjPf1DT+okWLc+mlV3RtK7U8YGl9t7Z47Vv2PPvp+7s19zW2rbh8CUcd+6jG9gc96c8b29718qM44z9WNLbf/K13N7ZddfkSHt4y353nbtfYJk2lE49fPGn1rOcCH8/MswEiYlY9rPkrby0zPwZ8rO73bqC5OK0kSeqqn9+sL6RKziPmAt/qZ+IRsVf9/0iJzHPHGqAkSTNdP3vWczJz1ciLzFwVEfP6nL4lMiVJmqB+kvXqiDgmM38AEBGLqE4Y68kSmZIkTVw/yfqNwOcj4pf1632A5w8uJEmS1Klnss7MyyPiMOBQqrPBr8vM5tM/JUnSpOq3ROaxwIJ6/KMjgsz8xMCikgrQenVitre3dk0Ybrk2eSL9oukCkB7xtl0rnQkbNjaXudxuVvN5qkG0tq/48l83tt24chkrvvycxvYFL/9kY9u7nrYLz/zX5vbfnveKxjapRD2TdUR8EngIsBzYWA9OwGQtSdIU6GfPejFweD83QZEkSZOvn+usVwIPHHQgkiSpu372rPcAro2Iy4B1IwMz85ltnSJiDnAxsEM9ny9k5l9NIFZJkmakfpL1O8c57XXAE+qbqGwHLImIr2fmsnFOT5KkGamfS7cuiogDgYMz81v13ctm9dEvqQp4AGxXP/zdW5KkMeqn6tarqEpY7paZD4mIg4GPZOYTe068KvpxJfBQ4MzM/LMu41gicxL7bm3xFt235aPRq2/bp2r1qlXMH0fM/fRrunJrkPG2/Qm5d/Uq5rWUqtww3HxJ2Lo1q9lh7vzG9mt/3nz34v12mcWtd29sbF/44N0b26Sp1G+JzH4Og78OOA64FCAzrx8p0NFLZm4EFkbEA4AvRcSRmbly1DhnAWdBVSKzqRRfqeUBS+u7tcVbct+2L7LLllzECY96bEvf5vkuW3oRJ5zY3Hci/Zqus+4Vb9t11pd9/2KOe+RjGts3bGzue+Wy77HohOa7Dv/6nnWNbTeuXMZBR57Q2P7sj3T/Yg/VddZnfO3uxvbfnvfcxjapRP2cDb4uM9ePvIiI2YzxcHZm3gV8FzhpTNFJkqS+kvVFEfHnwNyIeDLweeCrvTpFxJ71HjURMRd4EnDdRIKVJGkm6idZvw34DXA18EfA14C/6KPfPsB3IuIq4HLggsw8f7yBSpI0U/VzNvgwcHb96FtmXgUcPc64JElSrZ97g99Il9+oM/PBA4lIkiRtpt97g4+YAzwP2G0w4UiSpNH6OQx+x6hBH4yIJcA7JjuYDcPJ3fd2L5W9saUNYLjlWpkNG5M7V69vbG+7zGbDxuS3q5r7tunVt+2U+g0bkzsa+rZdkNdrnvO2b76fzfAwrF3ffG3qDtu1nOIwwJKRbcsps/2yo9XrNjS2bRxO7lnTvE21XZK0YTi5c3Vz37aYNw4nd7ZsyxPp17Rt9Ip31lDzVjWcyep1zdvF3JZtKgKGWqb9oD3mNbbdOnuotb2tzOUlS77r5VnapvRzGPyYjpdDVHvaOw0sIkmStJl+DoP/Q8fzDcBNwB8MJBpJkrSFfg6DP34qApEkSd31cxj8zW3tmfmPkxeOJEkard+zwY8FvlK/fgZVnepfDCooSZK0ST/Jeg/gmMz8HUBEvBP4fGa+cpCBSZKkSj8lMq8DjsrMdfXrHYAVmXnYpAQwqkTmpz5zbtfx1qxexdyWUnttb2PtvauYM298ZRS3tr69+g01lWWidznDlitwpqVkZD992y7p29q2qUHOs2Wz6Lmc2rapXuunbb7TVTpVmkqTWSLzk8BlEfElqr+5zwY+McH47tdZIvOooxfl7y1+VNfxrr5iCU1t0P5HeeUVSzly8YktMTTHd82VSzliUXPfNr36tiWwa69cyuENfdvWaq95tl1n3aucYdt11oMsGdm2nC5dehHHt/Rtu876qsuX8PBjm7eptuusB7Vu2/TTr2nb6BVv23XWvT57bddZX3HJxSx+RHN5ze1nN29T01U6VSpRz0Iemfku4OXAncBdwMsz8939ziAiXhcRy+vHvuMPVZKkmamfPWuAecA9mfkfdenLgzLzxn46ZuaZwJnjjlCSpBmu5551RPwV8GfA2+tB2wGfGmRQkiRpk37qWT8beCawGiAzf4m3G5Ukacr0k6zXZ3XKeAJExPzBhiRJkjr185v15yLio8ADIuJVwCuAswcRzKyhaDxbeSia2wDWbxhubBuKYPtZzd9L2s4kj4DtZjWfKbuhpeLTSP8mq9Y0n6k8nMnqtc3tbf1WtfRru1Svqq7U3Hf2rO2ap0t79ate2tZBr8m2zbetithw9qoy1l5Nqu0M6jYRMLulb9M202tbhOaz7nvFO3+H5vc6FNHaPrvlsxXRfsa3pP70c2/wD0TEk4F7gEOAd2TmBQOPTJIkAX2eDZ6ZF0TED4DHAL8dbEiSJKlT4/GpiDg/Io6sn+8DrKQ6BP7JiHjjFMUnSdKM1/Zj0kGZubJ+/nLggsx8BnA8VdKWJElToC1Z39fx/InA1wDqgh7NZ3PVIuLjEfHriFjZa1xJktSsLVn/IiLeEBHPBo4BvgEQEXOpbozSyznASROOUJKkGa4tWZ8OHAG8DHh+Zt5VDz8B+I9eE87Mi/FkNEmSJqxnicwJTTxiAXB+Zh7ZMs5mJTI/c+55XcfrVWqv7W30Kv2YLTWSBllGse3a4nVrVrPD3LHff6ZXv7Zyhr3ibbtOd5BlLifSt+06+N7lRJvn22u7aDPevoOc57ZW5tISmdpaTGaJzIHqLJF5zKLFeWxDOb3LL7mYpjZovynK8suWsPC48ZXXXHH5Eo5qK6PYkgx6leb8XctNUW5cuYyDjjyhsX28/dpubnHtD77P4cc8srF9l3nNv35c9v2LOe6RzeunTa++bTdF6VWC8a7V6xvbrlt+CYctfERje9tNUXqVjWzTq29T8utV0hOavzz2mmfbdtFr/bTdFGW6ylxaIlPbGm8tJElS4fqpurXFrmG3YZIkaTD62bP+lz6HbSYizgUuAQ6NiFsi4vSxBidJklp+s46IRwCPBPaMiDd3NO0MNP/AVcvM0yYeniRJajvBbHtgx3qczvrV9wCnDjIoSZK0SWOyzsyLgIsi4pzMvHkqgslsPru6rQ2AXie+t7TPbTnrdyiCuS2lOdsuZ5o9FDyg5QzqneY0f1e6ZdYQ++46p2tb23L4xawh9tplh9aYmswaitYzvqPtGp0+2sfbt0dVyPbSjy3LeFZEa/vd997X2DY8TGs50bX3NV+dsGFjcseq5rPUm97PxuHkztXNMUFzOcrhYbi3pRzoPWuap3vfxmFuu2ttY/uOLctww8bkzpYz8ts+H2R7SdeJbG/S1qafS7fOiYgtPjGZ+YQBxCNJkkbpJ1m/peP5HOC5QPMuhSRJmlQ9k3VmXjlq0NKIuGhA8UiSpFH6uc56t47HHhHxVOCB/Uw8Ik6KiB9HxA0R8bYJRytJ0gzUz2HwK4GkOkVrA3AjVZGPVhExCzgTeDJwC3B5RHwlM68df7iSJM08/RwGP2ic0z4OuCEzfwYQEecBzwJM1pIkjUHPZB0Rc4DXAo+i2sNeAvxbZjZfy1HZD/hFx+tbgOPHGackSTNWzxKZEfE54HfAp+pBpwG7ZubzevR7HvDUzHxl/folwHGZ+YZR421WIvPTn+leIrNXmcs2vfq2Xa7Zszxgy3x7lelrW/Jt823rd++qVcwbZ7yllrmcSN+JlE7d2FZec80q5swdX/nT9WtXs/2c5jKmTdtjP2VTo2EN94q3rUzsIMuutl3337PM5YBKc0pTaTJLZB6amUd1vP5ORKzoo98twAEdr/cHfjl6pM4SmUcfsziPOeHRXSf2g2Xfo6kN2v84Lr90CQuPby4PuEPDjSSgd3nAthtyLFtyESc86rGN7W3JoG2+bTdFuXLZ91jUspza/jj2eq9tN6G4dOlFHH9i83ttM8i+a+9rvhFIr+2i7aYoN6xYxkOPai5F2nZTlJ9fcykPOqL5IFPTNnXTymUs6FE2temmKNcvv4SDW8qBbtjYHO/Prl7Gg3+veb5tN0XpVSa27aYovT4/bdujJTK1remnkMcPI+L+T2pEHA8s7aPf5cDBEXFQRGwPvAD4yvjClCRp5upnz/p44KUR8fP69YOAH0XE1UBm5sO7dcrMDRHxeuCbVIU/Pp6Z10xG0JIkzST9JOuTxjvxzPwa8LXx9pckSf0l67/LzJd0DoiIT44eJkmSBqOf36yP6HwREbOBRYMJR5Ikjda4Zx0Rbwf+HJgbEfew6UKJ9dRnb0+2oaCxHOXQUHNbz+kOwfwd+jmIsKUImD2rn+803Tq3n7E6u6X2Y9t8Z7cshqGAOS0lP1tLDtJ+dvuNv7m3sW39hmFuvr25/YNLb2xsO3H2Ws77cvPpDMfs33wJzu73rueTVzZXcH3pogMb2yLay6PO37V5m7l5drDvrnMb29v86idDPHiv9kuwuvnl7CEO3GPeuOZ50+xgnwd0L7nayy2zhzhg9/HNd/asYNf524+rb6/PjzSTNGahzPz7zNwJeH9m7pyZO9WP3TPz7VMYoyRJM1o/u5tfj4gtLr7NzIsHEI8kSRqln2T91o7nc6ju+X0l8ISBRCRJkjbTTyGPZ3S+jogDgPcNLCJJkrSZ8Zw5dQtw5GQHIkmSuuun6ta/sKl2xBCwEOjn3uCSJGkS9FN16w87Xm4AbsrMfu4N3l8Ao6punXte96pbE6miM5P69uzXsrp79V23obnYQ69KUr9atb6xbcdYz6psvrxn3vbNB4Bmb1jLhtnNlyTtPq95uj0rqg2oqtPA1q19J6WvNJUms+rWZ4GHUv2Z/2kfdazHpLPq1qJFi7OpUs5EqujMpL69+rV9OetV5ajtOutelaT+q/U661tYumH/xvZjHthynfVvf8Idux3S2P70luusly29iBNaKnYNtVxzXuK6te/k9JVK1LjLEhGzI+J9VL9R/ydVPetfRMT7IqK5rt2W03ldRCyvH/tOPGRJkmaWthPM3g/sBhyUmYsy82jgIcADgA/0O4PMPDMzF9aPLepZS5Kkdm3J+mTgVZn5u5EBmXkP8BrgaYMOTJIkVdqSdWaXHzgzcyOtpylJkqTJ1Jasr42Il44eGBEvBq4bXEiSJKlT29ngrwP+KyJeQXV70QSOBeYCz56C2CRJEi3JOjNvBY6PiCdQ1bQO4OuZeeFUBafJ11pysEdJwv12bb6e+bbZQ63tq9dtaGwbnpWt7df9ek1j2yKGW9t7vN3WdvWn9V4N2aO9dcI9Srq68jSD9HNv8G8D356CWCRJUhfjuTe4JEmaQiZrSZIKN9BkHREnRcSPI+KGiHjbIOclSdK2amDJOiJmAWcCvw8cDpwWEYcPan6SJG2rBrlnfRxwQ2b+LDPXA+cBzxrg/CRJ2ib1LJE57glHnAqclJmvrF+/BDg+M18/ajxLZE5i30HOc7hlU7l31SrmtfT9+Z3Nl1ftPHQf9ww314aZPav5Ep35rGc1zWUw99+5+XKynsvKEpn99Z1A2dUJzXdA60eaSpNZInO8us282+1LLZE5iX0HOc91921sbLti2fdYfMKjG9s//sWrG9ueNO82vnXvPo3te+60Q2PbIm7mSprLYJ76qMMa23qVBG27jndbW7cT6TuRsqttpmv9SCUa5GHwW4ADOl7vD1h1S5KkMRpksr4cODgiDoqI7YEXAF8Z4PwkSdomDewweGZuiIjXA98EZgEfz8xrBjU/SZK2VYP8zZrM/BrwtUHOQ5KkbZ13MJMkqXAma0mSCjfQw+Datuyw3azGtqFobz/7D45qbFu29C7OPrm5fffj39DY9q5XH89Hzzq/sf3vTvpQY1sCG1suHm+7vnuQGi+FGmC5yYH17WEiJVsnYlDvR5PD8qdbcs9akqTCmawlSSqcyVqSpMINukTmn0TEyoi4JiLeOMh5SZK0rRpkicwjgVdRVd86Cjg5Ig4e1PwkSdpWDXLP+mHAssy8NzM3ABcBzx7g/CRJ2iYNskTmw4AvA48A1gAXAldk5htGjWeJzEnsW2q8bZvZ6lWrmN/Sd/l1v2hs22+P+dx6++rG9oWHHdDY1mu+bVePTEfJyVLX7YT6FraMVYgZdOXWtJfIzMwfRcR7gQuAVcAKYEOX8SyROYl9S413uOV65mVLL+KEE5tLIT79ze3XWZ9x1qWN7b9Z9qLGtsu+fzHHPfIxje2zZzUfeJqOkpODLDc5XX2nqwyp11mXzeustzTQE8wy82OZeUxmPgb4LXD9IOcnSdK2aKB3MIuIvTLz1xHxIOA5VIfEJUnSGAz6dqNfjIjdgfuA12XmnQOenyRJ25xBl8h89CCnL0nSTOAdzCRJKpzJWpKkwg3sOuvxiIjfADc3NO8B3D7OSc+kvltbvPYte54zsa80lQ7MzD17jVRUsm4TEVdk5mL7ljdP+05N360t3q21r1QiD4NLklQ4k7UkSYXbmpL1WfYtdp72nZq+W1u8W2tfqThbzW/WkiTNVMXvWde3KpUkacYqOllHxNOACyNiv+mORZKk6VJsso6IpwIfAF6SmbdGxJTGGtNQoy0i9p6O+WpsXEeSplqRyToingJ8AriWqrQmmTk8xX8k961jGdf90yNilzGOvx/wF8Bp432fETF3PP3qvgdGxJzx9h/H/A6NiEdExHYRMWsM/Q6OiMURMWss/SZDROxfF6bZf5z9HzaGcbePiMPr50+MiH3GM8+JGO/yHe86mui6jYgjIuKx9TqStinFnWAWEU8E/g34a2BvYC/g/MxcUrdHjiHoiHgUcDhwdr/9IuL1wFOBa4BfAh/NzHVjmOdrgZ2Af8vMe/rsE8AfAkcAy4D/GuP7fD1wKLAKeE9m3j2GvnsBfwn8fWb+st9+4xURzwHeDdxaP64Azum1rCLiFKrt4gbgFuDHwH9m5urBRgwR8SzgbcCvgH2ArwPvzsz1ffZ/DfB04PTM/FUf4z8U+Nd6frsBL83MO8YZ/phExCGZ+ZP6+azM3DiGvuNaRxNdtxHx+8B7gZ8B21Et5//tN26peJlZ1AM4Fnhk/fxQ4G+BvwdO7Bgn+pjOUP3/S4EPAy/ps98pwMXAA4DvAB8eY/x/BFwKHFC/nt1Hn+iI9avAZXUcPeOt+70WuAjYj+qP+yeAg8cQ8xDwFaokP+j1ux3w2ZH1CTwXeD/wd8DOLf12p0qQh9evXwFcTnU0YqcBx/x44CfAonq7OITqC9W7RrazHv2fCayguq3gWOb7AeAe4PX161n9bhMTeK8nA/cCn+kYNqvPvuNaRxNdt8Dj6vVzXP36S8CTBr0t+/AxlY/iDoNn5uWZ+f2IGMrMH1MlnvuAkyPikfU4/exxPqT+/1PA94CjgZf2cYh5F+CDVMnyPuDNUO1t9JphfRj694F3APfWe1Nn1v83ysyMiBcBbwDOAL5PlSCe2yveiNgZOAZ4AVXi+2Hd9KGIOLhH333rvahh4PXA3hFxWK/3OQl2BkZi+xJwPrA98MKW97sB2BF4IEBmfpzqPvJ7UiWYQXok8KHMvBJYm9Ve5/Op1vWf99F/X+CzmXlzRGw3hvl+hOqL2Csi4kWZubHeVnYc6xvoR0TMp9oO3gisj4hPAWTmxj4PS493HU103f4K+KPMvCwiHggcD7w+Ij4aEad6joG2BcUl6xF1AiEzrwc+CawFXhARx/fqW1/udUFEvKSezhepktiLgJf3+PDeRLWnd3pmPiUz10fE/wFe2esPbWauAb5GdSTg48CBVIfSj4yI7XuEfSjwucy8Cngr1eHANwDPa4s3q0PHr6P6ueDZmXkS1eH0Y4GXNM23/sP8VuAjEfFqqsP266j2zgd2ElVm3gf8I/CciHh0vX6WAMuBR7X0uxv4NNX6e0lEvItqm7gWePIgYu1YBvtTFYYAWFcfGr4ZeBnwpIjYq8fyuhl4dEQcWr9/6vdwStv8M/OGzPwU8FfA/42Ip9fnc/zf8Z5L0WN+q6n2aj8DvAWY05mw++g/rnU00XWbmT/KzO/UL08H/jUzT6E6+vE8Nq07aes13bv2/T6Aw4C3A3v2Of4zgB8Ap3UM+xrwD8AuLf12pEomH6A6vPZS4ErgyD7nO4cqUe5Wvz6N6nD6vB79TgH+GziiY9gSqkOt/RwKPJjqCMLvAU+jOqLwoD5iPYbqsPQZVHsolwP7DXhdzqHagzsLeEzH8G8DC1v67UL1hes/gH/qGH4+LYfQJyHeJwLfAhbVr4eoDufvS/VFcH6P/jtTHeZ/N9We4mn1cn7oGGI4CbiK6vf9wwe5fjrmuXv9/j5Vvz4GOKxHn3Gto0Gt2/ozf8xULC8fPgb5mPRv54OSmddFxAey3jPpY/yvRsRG4D314enfAsPAB7Ll5KvMXBUR76f6nfGtwB3AyzJzZZ/zXQtcHhFDEXE61SHF0zLz3h5dv0uV5E+LiG8DIzH/S2b+ro9Z/5zqD9s/Up2Y9weZ+fM+Yv1BvWe9A1USWgg8CLh1rCfz9Ssz10bEp4EE3l4fel9Xx31bS7+7gU9HxLlZH3mJiJdSnYDV90lQ47CM6ovT8yOCrA6HD9cnL+5GlbgbZeY9EXEm8Cyqw9p3Ux25uaHfADLzGxFxZf38N+N8H2OSmXdExB8B74+I66h+M398jz7jWkeTsW5Hb68R8VyqbWrgJ01Kg1bc2eCTLSIeS3WW6b3A27I6zNxv3+3g/kO3Y53vPKrfNZdl5o/67LMv8Jz6sQH408y8eozxPhAYzsxbxxpzPY0zqE6EevV4+o9xXtsDJ1KdlLcW+OfM/GF7r836v4LqcO3zx7KcxiOqS+teCTwBuARYD5xK9UVsxRimsz1A9nkWeQki4k3AnwFPHutyHu86msi6jYgdgBdTnW/y/H6/aEsl2+aTNdyfODOr35Sncr7j2jOtf0+OzFw1gLCa5hmZmRHxAuDlwClTtbzqk5dyZI9qDP0OBLYbyx7qRNRHaBZTXdZ3O/D1rE6C3GZFxK7A56i+OPb9Rbej/7jW0UTWbf2l9cnAT7f19aOZY0Yka/WnPknqZOBG90Y0IiLm1D+ZSJomJmtJkgpX7KVbkiSpYrKWJKlwJmtJkgpnspYkqXAma2kKRcSkX44XEQsi4oUNbUMR8aGIWBkRV0fE5RFx0GTHIGmwtpo7mElqtAB4IdU9vUd7PtVtUR+eVU34/YGBlxSVNLncs5amQUQ8LiK+GxFfiIjrIuLTI8VAIuKmiHhvRFxWPx5aDz8nIk7tmMbIXvp7qAqFLK/vNtZpH+A4TVKOAAAU50lEQVS23FQY55bMvLPu/5SIuCQifhARn4+6mldEnFTHtKTeKz+/Hv7OiHhLx/xXRsSC+vmL61iXR1XtatZIjBHxrohYERHLImLvevjeEfGleviKqCvqNU1HmulM1tL0OZrq3vGHAw+muvXqiHsy8ziqWuwf7DGdtwHfy8yFmflPo9o+BzyjTn7/EBFHA0TEHlT1op+UmcdQFQh5c0TMAc6mKoTzaOqylW0i4mFUe/AnZuZCqnt5v6hunk91y92jqOrEv6oe/iHgonr4McA1PaYjzWgeBpemz2WZeQtARCynOpy9pG47t+P/0Qm4b5l5S0QcSnVP8ycAF0bE86gKxRwOLK136Lenuuf5YVR3sLu+jutTQK/7xD8RWERVwIZ62r+u29ZTFZiBqnrdSMnLJ1BVtCOr8pt3R8RLWqYjzWgma2n6rOt4vpHNP4/Z5fkG6qNh9SHzXjXSq86Z64CvA1+PiF9RlWP9H+CCzDytc9yIWDhq3p3un39tzkg34D8z8+1d+tzXcX/80e9xtLbpSDOah8GlMj2/4/9L6uc3Ue15QlVuc6Q05++AnbpNJCKOqau5ERFDwMOBm6nKfp7Y8Xv4vIg4BLgOOCgiHlJPojOZ30R1yJqIOAYYOav8QuDUiNirbtutLsTR5kLgNfX4syJi53FOR5oRTNZSmXaIiEuBPwFGTho7G3hsRFwGHM+ms7qvAjbUJ2qNPsFsL+CrEbFyZDzgw3VN7JcB50bEVVTJ+7C6YMergf8XEUuoEvuILwK71YfsXwP8BCAzr6X6/ft/6mldQHViW5s/AR4fEVdTHR4/YpzTkWYEC3lIhYmIm4DFmXl7AbE8DnhLZp483bFIM5l71pIkFc49a0mSCueetSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhTNZS5JUOJO1JEmFM1lLklQ4k7UkSYWbPd0BbK2e8tST8vbbb+85Xt7/T0NbUyOQzU1b9mydR8NI2dq1oHllY78thmdzHN2m0W39NPUYHdfo6XVvb5haH/27RwGZrUt6i+2m+zLqvkR79+3es7Vf9lgHjdtTl4XUOY0ub6zn563bwmhoG+v4m43V9uG9/7PQvrA3ax/jMur8wHVbh23jN85wi37dPtSjY+7Sp+2PScf8c81vvpmZJ3UJdkYyWY/THbffztJlV2z2YUmq7TlHfVCy48PZub13jpu5+bY9Mm7nZ6ez/6bpbt6/c16dn4tecXUddwzvazLnNdyREEbah7dYLtWA4dHLMGF4s2WyaZkNj1qmmckwm/6wZsewkfbO8TePa6RvR1tW/98f16hYhjvaR15nx/jDo99Xx7RHv66mPXreHbGNft35PnNTn8732fkec7P3sfm4nXEn3afV+T5H+nSuv67TaogrR01ry9ft4/c37pZ9h4f7j4UtprVlW2f7ZIw/nmlVgQ93fCCHNw3r+rrL86a+wyPtfY7f1F4/X7v8zD3Q/TwMLklS4UzWkiQVzmQtSVLhTNaSJBXOZC1JUuFM1pIkFc5kLUlS4UzWkiQVzmQtSVLhTNaSJBXOZC1JUuFM1pIkFc5kLUlS4UzWkiQVzmQtSVLhTNaSJBXOZC1JUuEiM6c7hq1SRHwD2GO64+hiD+D26Q6iQamxGdfYlRpbqXFBubGVGtftmXnSdAdRCpP1NiYirsjMxdMdRzelxmZcY1dqbKXGBeXGVmpc2pyHwSVJKpzJWpKkwpmstz1nTXcALUqNzbjGrtTYSo0Lyo2t1LjUwd+sJUkqnHvWkiQVzmQtSVLhTNZbqYg4KSJ+HBE3RMTburQ/JiJ+EBEbIuLUguJ6c0RcGxFXRcSFEXFgQbH9cURcHRHLI2JJRBxeQlwd450aERkRU3aZTR/L7GUR8Zt6mS2PiFeWEFc9zh/U29o1EfGZEuKKiH/qWFY/iYi7piKuPmN7UER8JyJ+WH8+nzZVsakPmeljK3sAs4CfAg8GtgdWAIePGmcB8HDgE8CpBcX1eGBe/fw1wGcLim3njufPBL5RQlz1eDsBFwPLgMUFLbOXAR+einjGGNfBwA+BXevXe5UQ16jx3wB8vKBldhbwmvr54cBNU7lefbQ/3LPeOh0H3JCZP8vM9cB5wLM6R8jMmzLzKmC4sLi+k5n31i+XAfsXFNs9HS/nA1Nx9mXPuGp/C7wPWDsFMY01tqnWT1yvAs7MzDsBMvPXhcTV6TTg3CmIC/qLLYGd6+e7AL+cotjUB5P11mk/4Bcdr2+ph023scZ1OvD1gUa0SV+xRcTrIuKnVInx/5QQV0QcDRyQmedPQTyd+l2fz60Pm34hIg4oJK5DgEMiYmlELIuIqbhtZd/bf/3zz0HAt6cgLugvtncCL46IW4CvUe35qxAm661TdBlWwjV4fccVES8GFgPvH2hEHbPsMmyL2DLzzMx8CPBnwF8MPKoecUXEEPBPwJ9OQSyj9bPMvgosyMyHA98C/nPgUfUX12yqQ+GPo9qD/feIeEABcY14AfCFzNw4wHg69RPbacA5mbk/8DTgk/X2pwK4IrZOtwCdezD7U8Yhq77iiognAWcAz8zMdSXF1uE84JSBRlTpFddOwJHAdyPiJuAE4CtTdJJZz2WWmXd0rMOzgUUlxFWP8+XMvC8zbwR+TJW8pzuuES9g6g6BQ3+xnQ58DiAzLwHmUGaxohnJZL11uhw4OCIOiojtqT74X5nmmKCPuOpDuh+lStRT8TviWGLr/GP+dOD66Y4rM+/OzD0yc0FmLqD6nf+ZmXnFdMcGEBH7dLx8JvCjEuIC/pvqZEYiYg+qw+I/KyAuIuJQYFfgkgHHM9bYfg48sY7xYVTJ+jdTGKPaTPcZbj7G96A6TPUTqjM8z6iH/Q3VH3KAY6m+Ta8G7gCuKSSubwG/ApbXj68UtMz+Gbimjus7wBElxDVq3O8yRWeD97nM/r5eZivqZXZYIXEF8I/AtcDVwAtKiKt+/U7gPVO1DsewzA4HltbrcjnwlKmO0Ufzw9uNSpJUOA+DS5JUOJO1JEmFM1lLklQ4k7XuFxHPru89fVjHsAURsbJHv57jTKb6ftQfnqRpRUR8OyJ2rl9vrO/bvDIiPh8R88Y4vVVjHP+c6HLv9ohYHBEfqp/f/37r+5e/tGP4vmOZ31hFxOMi4pETnMafj6PP8yLiRxHxnVHDF0TECzteT2hbqJf/4yLiuxGxYBz9D6u3lx9GxKKIeO14YxnDPN9Zv+9zIuJx9bDzRl3NoG2MyVqdTgOWUF3WMVM8DViRm241uiYzF2bmkcB64I87R66T+8A/N5l5RWZucQe1zPxIZn6ifvkyYKDJmuqmIhNK1sCYkzXVNb+vzczHjxq+AHjhlqNPm1Ooruc+muqqi4En6wb/BvzfaZq3poDJWgBExI7AiVR/JLsm6/rb/Jcj4ht19Z6/6mieFRFnR1Xh6H8iYm7d51URcXlErIiIL47eU42IoYi4qfPuUlFVBdo7Ip4REZfWey3fioi9u8S02Z5p555tRLy1nvdVEfHXDW/9RcCXG9q+Bzy03pv7UUT8K/AD4ICIOC2qCl0rI+K9o2L6h6gqnl0YEXv2sRyeFBHfi6oK08n1+I+LiC1uL1rvVb2lfs+LgU/Xe3ZPj4gvdYz35Ij4ry79n1gvz6sj4uMRsUM9/Kaorkce2asf2dP8Y+BN9TweXS/vj3SJd7M93Ig4v34P7wHm1v0/3SWeLZZjRLwDeBTwkYgYfYe79wCPrqf3pnrYvvU2eX1EvK9j2k+JiEvqdfH5ehsf7W6qL2W/BTZGxKz6Pa6s43pTPa2FUd229KqI+FJE7BpVVao3Aq+M6gjAe4CH1LG9v37/F0XE5+pl9Z6IeFFEXFZP+yH1tLtu5xHxoXpZEBFPjYiLo/qiuApY0xE7VNvqkyJidpf3qG3BdF875qOMB/Bi4GP18+8Dx9TPFwAr6+cvA24DdgfmAiupEsYCYAOwsB7vc8CL6+e7d8zj74A3dJn3PwMvr58fD3yrfr4r3H954SuBf+iI48P183PoqCoGrKr/fwpVFaGg+lJ6PvCYLvO+GdipS//ZVEn8NfX7GwZOqNv2pbqBxJ71eN8GTqnbEnhR/fwdHXF2XQ51/N+oYzyY6tr4OVR7tOd3eb/vBN5SP/8u9TXX9fu8Dtizfv0Z4Bmj3uscqvtDH1K//gTwxvr5TcAe9fPFwHdHz69HvPfHWI93PvC4zmXaZdm3Lcf739uoPvcvl45l8zOqwhNz6vV5ANWdty4G5tfj/Rnwjj4+B4uACzpeP6D+/yrgsfXzvwE+2GV9LKD+rHTEehewD7ADcCvw13Xbn3RMo2k7n0d1Dfvjqe7A9pAesV8ALJruvyU+BvNwz1ojTqO6xSb1/6c1jHdBVreYXAP8F9UeEMCNmbm8fn4l1R8ugCPrvbCrqfZij+gyzc8Cz6+fv6B+DdUtEb9Z931rQ98mT6kfP6TaGz6M7reb3C0zf9fxem5ELAeuoEokH6uH35yZy+rnx1Ils99k5gbg08Bj6rbhjvg/xabl07YcPpeZw5l5PVXiOYwxyswEPklViOEBwCPYskjKoVTr6Sf16//siHssJhxvrW05jsWFWd3pbS3VTVAOpLot6+HA0np9/mE9vJefAQ+OiH+JqvjHPRGxC1XSvqgeZyzL7fLMvC2rW7L+FPifevjVbPqMdN3Os6pO9yqqJPzhzPxpj3n9msH/LKJp4iETERG7A0+gSihJVfs2I6Lbb2Cj76Iz8rrzHt8bqfa8odoTOyUzV0TEy6j2Nka7hOpw855UvwH+XT38X4B/zMyvRHUizTu79N1A/XNORARVrV6o9jT/PjM/2qXPZv0jYigzR0qJrsnMhZ0jVJNldeegHtPsNLJ8zqF5OTQt07H6D6rCGmuBz9cJsFNb3PcvR6o91Dbd4u3s3880esUzFqO3vdn1tC/IzKYvnV1l5p0RcRTwVOB1wB8Ab2rv1Xdswx2vh9n097dtO/89qt/C+0nCc6gOj2sb5J61AE4FPpGZB2Z1D+oDgBvZtFfY6ckRsVtUv0mfQnV7wjY7AbdFxHZUe5RbqPcKv0R1e8gfZeYdddMuVIcOodoz6uYmNhWPeBawXf38m8ArRn6njIj9ImKvLv1/DDy4x3sY7VLgsRGxR0TMojoKMbLXNUS1PKE6EWpJ/bxtOTwvqt/uH1LH8uM+4/hdPV0AMvOXVMUZ/oLqy8Fo1wELIuKh9euXdMR9E5uW43Ob5tES703Awnr4AVT1k0fcV7/v0dqWY5Nu8XSzDDhx5L1GxLyIOKRXp/p3+6HM/CLwl1Q/B90N3BkRj65H61xu44lttK7beVRlNP8UOBr4/Yg4vsd0DqE6bK5tkMlaUP2R/NKoYV+k+1m3S6gOty4Hvpi9C0r8JdUf5QuokkWTz1L9bv7ZjmHvBD4fEd8Dbm/odzbVH/zLqH7vXg2Qmf9D9bvtJfXhxS/Q/Q/p/6P73n6jzLwNeDvVvbBXAD/IzJGT1FYDR0TElVRHK/6mHt62HH5M9cf/68Af14dz+3EO1UlYy+svT1AdSv5FZl7bJe61wMuplunVVHt3H6mb/xr453pZd5Zt/Crw7JETzFriXUr1Be9q4ANUPz2MOAu4avQJZj2WY5OrqI6GrOg4wWwLmfkbqt+zz42Iq6iSdz+H6/ejqnC2nGr5vr0e/ofA++tpLWTTeu2c5x1Uh91Xdjkxrs07GbWd10eJPkb1e/gvqU78/PeI6HrEoj4pbU29TLUN8t7g6lt9+HZxZr5+umOZLFFVjfpEZj55umOZDFGdkf3DzPxYz5HHN/1zqE7w+sIgpq/xqb+43DOo9a7p5561ZrR6T+TsqG+KsjWr9+YfTnVim2aWu6hOfNM2yj1rSZIK5561JEmFM1lLklQ4k7UkSYUzWUuSVDiTtSRJhfv/LXokBWl82BoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20791c5db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
